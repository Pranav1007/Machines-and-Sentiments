{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from rake_nltk import Rake\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#nltk.download('popular', quiet=True) # for downloading packages\n",
    "#!pip3 install rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = pd.read_csv(\"IMDb movies.csv\")\n",
    "df_ratings = pd.read_csv(\"IMDb ratings.csv\")\n",
    "with open('chatbot.txt','r', encoding='utf8', errors ='ignore') as fin:\n",
    "    raw = fin.read().lower()\n",
    "\n",
    "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw)# converts to list of words\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"how are you\",\"hey\",\"Hi Bot\", \"Hi\")\n",
    "GREETING_RESPONSES = [\"Hi\", \"Hey\", \"*nods*\", \"Hi there\", \"Hello\",\"Hi dude, Im good\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "def greeting(sentence):\n",
    "    \"\"\"If user's input is a greeting, return a greeting response\"\"\"\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal set: (36075, 13)\n",
      "Custom Set:  (9540, 13)\n"
     ]
    }
   ],
   "source": [
    "# Taking a look at all the columns in the dataframe(s)\n",
    "#print(\"Columns for Movie Details: \\n\",df_movies.columns)\n",
    "#print(\"Columns for Movie Rating statistics: \\n\", df_ratings.columns)\n",
    "\n",
    "# Taking a look at the Movie Details Dataframe\n",
    "#df_movies.head()\n",
    "\n",
    "# Taking a look at the Movie Rating Statistics Dataframe\n",
    "#df_ratings.head()\n",
    "\n",
    "# Removing all the unwanted columns from the two Dataframes\n",
    "df_movies = df_movies[['imdb_title_id','title', 'duration', 'year', 'genre', 'language', 'actors', 'director','description']]\n",
    "df_ratings = df_ratings[['imdb_title_id', 'mean_vote', 'weighted_average_vote','median_vote', 'total_votes']]\n",
    "\n",
    "#Again Taking a look at all the columns in the dataframe(s) after dropping unwanted columns\n",
    "#print(\"Columns for Movie Details: \\n\",df_movies.columns)\n",
    "#print(\"Columns for Movie Rating statistics: \\n\", df_ratings.columns)\n",
    "\n",
    "# Merging the two dataframes and dropping all the nan values\n",
    "df = pd.merge(df_movies, df_ratings, on='imdb_title_id')\n",
    "#print(\"Shape, Before dropping Nan Values: \",df.shape)\n",
    "df.dropna(inplace = True)\n",
    "dfm = df.copy()\n",
    "#print(\"Shape, After dropping Nan Values: \",df.shape)\n",
    "\n",
    "df2 = df[df['language'].str.contains(r'English')]\n",
    "#print(df2.shape)\n",
    "df2 = df2[(df2['mean_vote'] >= 6) & (df['total_votes'] >= 1000)] # Take all English Movies with Rating greater than 6\n",
    "#print(df2.shape)\n",
    "df2 = df2[df2['year'] >= 2000]\n",
    "#print(df2.shape)\n",
    "df2[df2['title'].str.contains('123')]\n",
    "\n",
    "df3 = df[df['language'].str.contains(r'Tamil|Kannada|Telugu|Hindi|Malayalam')]\n",
    "#df3.shape\n",
    "df3 = df3[(df3['mean_vote'] >= 5) & (df3['total_votes'] >= 500)]\n",
    "df3[df3['title'].str.contains('Student')]\n",
    "#df3.shape\n",
    "\n",
    "\n",
    "df = pd.concat([df2,df3])\n",
    "df = df.apply(lambda x: x.str.lower() if(x.dtype == 'O') else x)\n",
    "df.shape\n",
    "\n",
    "dfm = dfm[(dfm['mean_vote'] >= 5) & (dfm['total_votes'] >= 500)]\n",
    "dfm = dfm.sort_values(by=['mean_vote'],ascending = False)\n",
    "dfm = dfm.apply(lambda x: x.str.lower() if(x.dtype == 'O') else x)\n",
    "print(\"Normal set:\",dfm.shape)\n",
    "print(\"Custom Set: \",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So we already have a list of some of the popular movies since the 2000s in English, Hindi, Tamil and some other Indian regional languages.\n",
      "Do you have any preferences?\n",
      "Want to make your own list?\n",
      "No\n",
      "(9540, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"So we already have a list of some of the popular movies since the 2000s in English, Hindi, Tamil and some other Indian regional languages.\")\n",
    "print(\"Do you have any preferences?\")\n",
    "print(\"Want to make your own list?\")\n",
    "ch = input().lower()\n",
    "if ('yes' in ch) | ('yea' in ch) | ('ya' in ch) | ('ye' in ch):\n",
    "    # Accepting user input to identify similar movies of their interest\n",
    "    gen = input(\"Cool!!\\nEnter Preferred genre(s) (if more than one please use a comma)(Type No if not): \").lower()\n",
    "    df2 = dfm.copy()\n",
    "    if gen != 'no':\n",
    "        gen = [x.strip() for x in gen.split(',')]\n",
    "        df2 = dfm[dfm['genre'].str.contains(gen[0])]\n",
    "        \n",
    "        for l in gen[1:]:\n",
    "            df2 = df2.append(dfm[dfm['genre'].str.contains(l)])\n",
    "        df2 = df2.drop_duplicates(subset=['title'], keep = False)\n",
    "    lang = input(\"Any Preferred Language(s) (if more than one please use a comma)(Type No if not): \").lower()\n",
    "    df3 = df2.copy()\n",
    "    if lang != 'no' :\n",
    "        lang = [x.strip() for x in lang.split(',')]\n",
    "        df3 = df2[df2['language'].str.contains(lang[0])]\n",
    "        for l in lang[1:]:\n",
    "            df3 = df3.append(df2[df2['language'].str.contains(l)])\n",
    "        df3 = df3.drop_duplicates(subset = ['title'],keep=False)\n",
    "else:\n",
    "    df3 = df\n",
    "print(df3.shape)\n",
    "df3 = df3.sort_values(by=['mean_vote'],ascending = False)\n",
    "if df3.shape[0] > 10000:\n",
    "    df3 = df3[:10000]\n",
    "if df3.shape[0] == 0:\n",
    "    print(\"I'm sorry but you have not selected any movies. Please try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Matrix to Recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "df3['Key_words'] = ''\n",
    "r = Rake()\n",
    "for index, row in df.iterrows():\n",
    "    r.extract_keywords_from_text(row['description'])\n",
    "    key_words_dict_scores = r.get_word_degrees()\n",
    "    row['Key_words'] = list(key_words_dict_scores.keys())\n",
    "    df3['Key_words'][index] = row['Key_words']\n",
    "\n",
    "df3['genre'] = df3['genre'].map(lambda x: x.split(','))\n",
    "for index, row in df3.iterrows():\n",
    "    row['genre'] = [x.lower().replace(' ','') for x in row['genre']]\n",
    "\n",
    "df3['Bag_of_words'] = ''\n",
    "columns = ['genre', 'Key_words']\n",
    "for index, row in df3.iterrows():\n",
    "    words = ''\n",
    "    for col in columns:\n",
    "        words += ' '.join(row[col]) + ' '\n",
    "    row['Bag_of_words'] = words\n",
    "    df3['Bag_of_words'][index] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn = df3[['title','Bag_of_words']]\n",
    "\n",
    "count = CountVectorizer()\n",
    "count_matrix = count.fit_transform(dfn['Bag_of_words'])\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_similarity_n_space(m1, m2, batch_size=10000):\n",
    "    assert m1.shape[1] == m2.shape[1]\n",
    "    ret = np.ndarray((m1.shape[0], m2.shape[0]))\n",
    "    for row_i in range(0, int(m1.shape[0] / batch_size) + 1):\n",
    "        start = row_i * batch_size\n",
    "        end = min([(row_i + 1) * batch_size, m1.shape[0]])\n",
    "        if end <= start:\n",
    "            break # cause I'm too lazy to elegantly handle edge cases\n",
    "        rows = m1[start: end]\n",
    "        sim = cosine_similarity(rows, m2) # rows is O(1) size\n",
    "        ret[start: end] = sim\n",
    "    return ret\n",
    "\n",
    "csmain = cosine_similarity_n_space(count_matrix, count_matrix)\n",
    "indices = pd.Series(df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
