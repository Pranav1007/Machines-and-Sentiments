{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from rake_nltk import Rake\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('popular', quiet=True) # for downloading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = pd.read_csv(\"/Users/saishashetty/Downloads/425324-810443-bundle-archive/IMDb movies.csv\")\n",
    "df_ratings = pd.read_csv(\"/Users/saishashetty/Downloads/425324-810443-bundle-archive/IMDb ratings.csv\")\n",
    "with open('/Users/saishashetty/Downloads/Building-a-Simple-Chatbot-in-Python-using-NLTK-master/chatbot.txt','r', encoding='utf8', errors ='ignore') as fin:\n",
    "    raw = fin.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw)# converts to list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"how are you\",\"hey\")\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\",\"Im good\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "def greeting(sentence):\n",
    "    \"\"\"If user's input is a greeting, return a greeting response\"\"\"\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdb_title_id</th>\n",
       "      <th>weighted_average_vote</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>mean_vote</th>\n",
       "      <th>median_vote</th>\n",
       "      <th>votes_10</th>\n",
       "      <th>votes_9</th>\n",
       "      <th>votes_8</th>\n",
       "      <th>votes_7</th>\n",
       "      <th>votes_6</th>\n",
       "      <th>...</th>\n",
       "      <th>females_30age_avg_vote</th>\n",
       "      <th>females_30age_votes</th>\n",
       "      <th>females_45age_avg_vote</th>\n",
       "      <th>females_45age_votes</th>\n",
       "      <th>top1000_voters_rating</th>\n",
       "      <th>top1000_voters_votes</th>\n",
       "      <th>us_voters_rating</th>\n",
       "      <th>us_voters_votes</th>\n",
       "      <th>non_us_voters_rating</th>\n",
       "      <th>non_us_voters_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0000574</td>\n",
       "      <td>6.1</td>\n",
       "      <td>537</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>54</td>\n",
       "      <td>17</td>\n",
       "      <td>55</td>\n",
       "      <td>121</td>\n",
       "      <td>122</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>64.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>309.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0001892</td>\n",
       "      <td>5.9</td>\n",
       "      <td>171</td>\n",
       "      <td>6.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>41</td>\n",
       "      <td>52</td>\n",
       "      <td>...</td>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>29.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0002101</td>\n",
       "      <td>5.2</td>\n",
       "      <td>420</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>60</td>\n",
       "      <td>89</td>\n",
       "      <td>...</td>\n",
       "      <td>5.5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>57.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>197.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0002130</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>6.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>194</td>\n",
       "      <td>208</td>\n",
       "      <td>386</td>\n",
       "      <td>571</td>\n",
       "      <td>308</td>\n",
       "      <td>...</td>\n",
       "      <td>7.3</td>\n",
       "      <td>74.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>75.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>452.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1076.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0002199</td>\n",
       "      <td>5.7</td>\n",
       "      <td>438</td>\n",
       "      <td>5.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>28</td>\n",
       "      <td>15</td>\n",
       "      <td>42</td>\n",
       "      <td>75</td>\n",
       "      <td>114</td>\n",
       "      <td>...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>56.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>161.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>164.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  imdb_title_id  weighted_average_vote  total_votes  mean_vote  median_vote  \\\n",
       "0     tt0000574                    6.1          537        6.3          6.0   \n",
       "1     tt0001892                    5.9          171        6.1          6.0   \n",
       "2     tt0002101                    5.2          420        5.2          5.0   \n",
       "3     tt0002130                    7.0         2019        6.9          7.0   \n",
       "4     tt0002199                    5.7          438        5.8          6.0   \n",
       "\n",
       "   votes_10  votes_9  votes_8  votes_7  votes_6  ...  females_30age_avg_vote  \\\n",
       "0        54       17       55      121      122  ...                     6.0   \n",
       "1         5        6       17       41       52  ...                     5.8   \n",
       "2        12        8       16       60       89  ...                     5.5   \n",
       "3       194      208      386      571      308  ...                     7.3   \n",
       "4        28       15       42       75      114  ...                     4.8   \n",
       "\n",
       "   females_30age_votes  females_45age_avg_vote  females_45age_votes  \\\n",
       "0                 19.0                     6.6                 14.0   \n",
       "1                  4.0                     6.5                  8.0   \n",
       "2                 14.0                     6.2                 20.0   \n",
       "3                 74.0                     7.4                 75.0   \n",
       "4                 10.0                     6.5                 15.0   \n",
       "\n",
       "   top1000_voters_rating  top1000_voters_votes  us_voters_rating  \\\n",
       "0                    6.3                  64.0               6.0   \n",
       "1                    5.9                  29.0               6.2   \n",
       "2                    4.9                  57.0               5.5   \n",
       "3                    7.0                 126.0               7.1   \n",
       "4                    5.7                  56.0               5.9   \n",
       "\n",
       "   us_voters_votes  non_us_voters_rating  non_us_voters_votes  \n",
       "0             89.0                   6.2                309.0  \n",
       "1             27.0                   6.0                114.0  \n",
       "2            197.0                   4.7                103.0  \n",
       "3            452.0                   7.0               1076.0  \n",
       "4            161.0                   5.7                164.0  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking a look at all the columns in the dataframe(s)\n",
    "#print(\"Columns for Movie Details: \\n\",df_movies.columns)\n",
    "#print(\"Columns for Movie Rating statistics: \\n\", df_ratings.columns)\n",
    "\n",
    "# Taking a look at the Movie Details Dataframe\n",
    "df_movies.head()\n",
    "\n",
    "# Taking a look at the Movie Rating Statistics Dataframe\n",
    "df_ratings.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all the unwanted columns from the two Dataframes\n",
    "df_movies = df_movies[['imdb_title_id','title', 'duration', 'year', 'genre', 'language', 'actors', 'director','description']]\n",
    "df_ratings = df_ratings[['imdb_title_id', 'mean_vote', 'weighted_average_vote','median_vote', 'total_votes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again Taking a look at all the columns in the dataframe(s) after dropping unwanted columns\n",
    "#print(\"Columns for Movie Details: \\n\",df_movies.columns)\n",
    "#print(\"Columns for Movie Rating statistics: \\n\", df_ratings.columns)\n",
    "\n",
    "# Merging the two dataframes and dropping all the nan values\n",
    "df = pd.merge(df_movies, df_ratings, on='imdb_title_id')\n",
    "#print(\"Shape, Before dropping Nan Values: \",df.shape)\n",
    "df.dropna(inplace = True)\n",
    "#print(\"Shape, After dropping Nan Values: \",df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all those movies which have a rating of less than 5 and if the number of reviewers are less than 500\n",
    "df = df[(df['mean_vote'] >= 5) & (df['total_votes'] >= 500)]\n",
    "#print(\"Shape of the dataframe after dropping selected movies: \",df.shape)\n",
    "\n",
    "# Selecting only the movies available in English, Tamil, Hindi, \n",
    "\n",
    "# When you are running code in your system\n",
    "# comment this line below and run it\n",
    "#df = df[df['language'].str.contains(r'Tamil|Kannada|Telugu|Hindi|Malayalam')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True,inplace=True)\n",
    "df = df.apply(lambda x: x.astype(str).str.lower())\n",
    "\n",
    "df['Key_words'] = ''\n",
    "r = Rake()\n",
    "for index, row in df.iterrows():\n",
    "    r.extract_keywords_from_text(row['description'])\n",
    "    key_words_dict_scores = r.get_word_degrees()\n",
    "    row['Key_words'] = list(key_words_dict_scores.keys())\n",
    "    df['Key_words'][index] = row['Key_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genre'] = df['genre'].map(lambda x: x.split(','))\n",
    "df['actors'] = df['actors'].map(lambda x: x.split(',')[:3])\n",
    "df['director'] = df['director'].map(lambda x: x.split(','))\n",
    "for index, row in df.iterrows():\n",
    "    row['genre'] = [x.lower().replace(' ','') for x in row['genre']]\n",
    "    row['actors'] = [x.lower().replace(' ','') for x in row['actors']]\n",
    "    row['director'] = [x.lower().replace(' ','') for x in row['director']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Bag_of_words'] = ''\n",
    "columns = ['genre', 'director', 'actors', 'language', 'Key_words']\n",
    "for index, row in df.iterrows():\n",
    "    words = ''\n",
    "    for col in columns:\n",
    "        words += ' '.join(row[col]) + ' '\n",
    "    row['Bag_of_words'] = words\n",
    "    df['Bag_of_words'][index] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df[['title','Bag_of_words']]\n",
    "\n",
    "count = CountVectorizer()\n",
    "count_matrix = count.fit_transform(df3['Bag_of_words'])\n",
    "cosine_sim = cosine_similarity(count_matrix, count_matrix)\n",
    "\n",
    "indices = pd.Series(df['title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(title, num=10, cosine_sim = cosine_sim):\n",
    "    recommended_movies = []\n",
    "    idx = indices[indices == title].index[0]\n",
    "    print(idx)\n",
    "    score_series = pd.Series(cosine_sim[idx]).sort_values(ascending = False)\n",
    "    top_10_indices = list(score_series.iloc[1:num].index)\n",
    "\n",
    "    for i in top_10_indices:\n",
    "        recommended_movies.append(list(df['title'])[i])\n",
    "\n",
    "    return recommended_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. Type 'help' for guidance, 'review' to review a movie and let others know your opinion, 'recommend' so we can recommend you a movie based on your preference :)\n",
      "help\n",
      "ROBO: i am here to help you, you can either review a movie or you can get recommendations based on your choice\n",
      "recommend\n",
      "ROBO: Alright...\n",
      "Enter a movie based on which we could recommend one to you: king kong\n",
      "Similar movies are\n",
      "543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the son of kong',\n",
       " 'the most dangerous game',\n",
       " 'the last days of pompeii',\n",
       " 'c.h.u.d.',\n",
       " 'the amazing spider-man 2',\n",
       " 'war of the worlds',\n",
       " 'mad doctor of blood island',\n",
       " '2019 - dopo la caduta di new york',\n",
       " 'vamps']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"ROBO: My name is Robo. Type 'help' for guidance, 'review' to review a movie and let others know your opinion, 'recommend' so we can recommend you a movie based on your preference :)\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='recommend'):\n",
    "        if(user_response!='review'):\n",
    "            if(user_response=='thanks' or user_response=='thank you' ):\n",
    "                flag=False\n",
    "                print(\"ROBO: You are welcome..\")\n",
    "            else:\n",
    "                if(greeting(user_response)!=None):\n",
    "                    print(\"ROBO: \"+greeting(user_response))\n",
    "                else:\n",
    "                    print(\"ROBO: \",end=\"\")\n",
    "                    print(response(user_response))\n",
    "                    sent_tokens.remove(user_response)\n",
    "        else:\n",
    "            flag=False\n",
    "            print(\"ROBO: Alright...\")  \n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Alright...\")      \n",
    "        \n",
    "string = input(\"Enter a movie based on which we could recommend one to you: \")\n",
    "#num = int(input(\"How many such similar movies do you want?\"))\n",
    "print(\"Similar movies are\")\n",
    "string = string.lower()\n",
    "recommend(string)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
